{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- V1 : LGBM STACKING \n",
    "- V2 : LGBM, MLP16 STACKING\n",
    "- V3 : V2 + pred 5 score 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import svm, neighbors, linear_model, neural_network\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\n",
    "\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, neighbors, linear_model\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern, RationalQuadratic\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD, PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 3.5 s, total: 22.8 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = [c for c in train_df.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n",
    "\n",
    "magic_variance_over2 = {}\n",
    "for magic in sorted(train_df['wheezy-copper-turtle-magic'].unique()):\n",
    "    temp = train_df.loc[train_df['wheezy-copper-turtle-magic']==magic]\n",
    "    std = temp[train_columns].std()\n",
    "    magic_variance_over2[magic] = list(std.index.values[np.where(std >2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hist_model(object):\n",
    "    \n",
    "    def __init__(self, bins=50):\n",
    "        self.bins = bins\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \n",
    "        bin_hight, bin_edge = [], []\n",
    "        \n",
    "        for var in X.T:\n",
    "            # get bins hight and interval\n",
    "            bh, bedge = np.histogram(var, bins=self.bins)\n",
    "            bin_hight.append(bh)\n",
    "            bin_edge.append(bedge)\n",
    "        \n",
    "        self.bin_hight = np.array(bin_hight)\n",
    "        self.bin_edge = np.array(bin_edge)\n",
    "   \n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        scores = []\n",
    "        for obs in X:\n",
    "            obs_score = []\n",
    "            for i, var in enumerate(obs):\n",
    "                # find wich bin obs is in\n",
    "                bin_num = (var > self.bin_edge[i]).argmin()-1\n",
    "                obs_score.append(self.bin_hight[i, bin_num]) # find bin hitght\n",
    "            \n",
    "            scores.append(np.mean(obs_score))\n",
    "        \n",
    "        return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Lasso, LassoLars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "debug = True\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svnu_params = {'probability':True, 'kernel':'poly','degree':4,'gamma':'auto','nu':0.4,'coef0':0.08, 'random_state':4}\n",
    "svnu2_params = {'probability':True, 'kernel':'poly','degree':2,'gamma':'auto','nu':0.4,'coef0':0.08, 'random_state':4}\n",
    "svc_params = {'probability':True,'kernel':'poly','degree':4,'gamma':'auto', 'random_state':4}\n",
    "lr_params = {'solver':'liblinear','penalty':'l1','C':0.05,'n_jobs':-1, 'random_state':42}\n",
    "mlp16_params = {'activation':'relu','solver':'lbfgs','tol':1e-06, 'hidden_layer_sizes':(16, ), 'random_state':42}\n",
    "mlp128_params = {'activation':'relu','solver':'lbfgs','tol':1e-06, 'hidden_layer_sizes':(128, ), 'random_state':42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oofs(random_state):\n",
    "    oof_nusvc = np.zeros(len(train_df))\n",
    "    preds_nusvc = np.zeros(len(test_df))\n",
    "\n",
    "    oof_nusvc2 = np.zeros(len(train_df))\n",
    "    preds_nusvc2 = np.zeros(len(test_df))\n",
    "\n",
    "    oof_qda = np.zeros(len(train_df))\n",
    "    preds_qda = np.zeros(len(test_df))\n",
    "\n",
    "    oof_svc = np.zeros(len(train_df))\n",
    "    preds_svc = np.zeros(len(test_df))\n",
    "    \n",
    "    oof_knn = np.zeros(len(train_df))\n",
    "    preds_knn = np.zeros(len(test_df))\n",
    "    \n",
    "    oof_lr = np.zeros(len(train_df))\n",
    "    preds_lr = np.zeros(len(test_df))\n",
    "    \n",
    "    cols = [c for c in train_df.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n",
    "\n",
    "    for i in tqdm_notebook(range(512)):\n",
    "\n",
    "        # each magic\n",
    "        train = train_df[train_df['wheezy-copper-turtle-magic'] == i]\n",
    "        test = test_df[test_df['wheezy-copper-turtle-magic'] == i]\n",
    "\n",
    "        # for oof\n",
    "        train_idx_origin = train.index\n",
    "        test_idx_origin = test.index\n",
    "\n",
    "\n",
    "        # start point\n",
    "\n",
    "        # new cols\n",
    "        cols = magic_variance_over2[i]\n",
    "\n",
    "        X_train = train.reset_index(drop=True)[cols].values\n",
    "        y_train = train.reset_index(drop=True).target\n",
    "\n",
    "        X_test = test[cols].values\n",
    "\n",
    "        # vstack\n",
    "        data = np.vstack([X_train, X_test])\n",
    "        \n",
    "        # PCA\n",
    "        data = KernelPCA(n_components=len(cols), kernel='cosine', random_state=random_state).fit_transform(data)\n",
    "        \n",
    "        # Bad\n",
    "        '''\n",
    "        gmm_pred = np.zeros((len(data), 5))\n",
    "        for j in range(5):\n",
    "            gmm = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data)\n",
    "            gmm_pred[:, j] += gmm.predict(data)\n",
    "        '''\n",
    "          \n",
    "        # original\n",
    "        gmm = GMM(n_components=5, random_state=random_state, max_iter=1000).fit(data)\n",
    "        gmm_pred = gmm.predict_proba(data)\n",
    "        gmm_score = gmm.score_samples(data)\n",
    "        gmm_label = gmm.predict(data)\n",
    "        \n",
    "        hist = hist_model(); hist.fit(data)\n",
    "        hist_pred = hist.predict(data).reshape(-1, 1)\n",
    "\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "\n",
    "        # HOXI\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        \n",
    "        # Add Some Features\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        data = np.hstack([data, hist_pred, gmm_score.reshape(-1, 1)])\n",
    "        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n",
    "        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n",
    "\n",
    "        # STANDARD SCALER\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "\n",
    "        # new train/test\n",
    "        X_train = data[:X_train.shape[0]]\n",
    "        X_test = data[X_train.shape[0]:]\n",
    "\n",
    "        fold = StratifiedKFold(n_splits=5, random_state=random_state)\n",
    "        for tr_idx, val_idx in fold.split(X_train, gmm_label[:X_train.shape[0]]):\n",
    "            \n",
    "            # NuSVC 1\n",
    "            clf = svm.NuSVC(**svnu_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_nusvc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_nusvc[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "\n",
    "            # NuSVC 2\n",
    "            clf = svm.NuSVC(**svnu2_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_nusvc2[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_nusvc2[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "\n",
    "\n",
    "            # qda 3\n",
    "            clf = QuadraticDiscriminantAnalysis(reg_param=0.111)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_qda[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_qda[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "\n",
    "            # SVC 4\n",
    "            clf = svm.SVC(**svc_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_svc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_svc[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "            \n",
    "            # knn 8\n",
    "            clf = KNeighborsClassifier(n_neighbors=16)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_knn[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_knn[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits   \n",
    "            \n",
    "            # LR 5\n",
    "            clf = linear_model.LogisticRegression(**lr_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_lr[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_lr[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "            \n",
    "    oof_train = pd.DataFrame()\n",
    "\n",
    "    oof_train['nusvc'] = oof_nusvc\n",
    "    oof_train['nusvc2'] = oof_nusvc2\n",
    "    oof_train['qda'] = oof_qda\n",
    "    oof_train['svc'] = oof_svc\n",
    "    oof_train['knn'] = oof_knn\n",
    "    oof_train['lr'] = oof_lr\n",
    "    \n",
    "    oof_test = pd.DataFrame()\n",
    "\n",
    "    oof_test['nusvc'] = preds_nusvc\n",
    "    oof_test['nusvc2'] = preds_nusvc2\n",
    "    oof_test['qda'] = preds_qda\n",
    "    oof_test['svc'] = preds_svc\n",
    "    oof_test['knn'] = preds_knn\n",
    "    oof_test['lr'] = preds_lr\n",
    "\n",
    "    print('nusvc', roc_auc_score(train_df['target'], oof_nusvc))\n",
    "    print('nusvc2', roc_auc_score(train_df['target'], oof_nusvc2))\n",
    "    print('qda', roc_auc_score(train_df['target'], oof_qda))\n",
    "    print('svc', roc_auc_score(train_df['target'], oof_svc))\n",
    "    print('knn', roc_auc_score(train_df['target'], oof_knn))\n",
    "    print('knn', roc_auc_score(train_df['target'], oof_lr))\n",
    "    \n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oofs_2(random_state):\n",
    "    oof_nusvc = np.zeros(len(train_df))\n",
    "    preds_nusvc = np.zeros(len(test_df))\n",
    "\n",
    "    oof_nusvc2 = np.zeros(len(train_df))\n",
    "    preds_nusvc2 = np.zeros(len(test_df))\n",
    "\n",
    "    oof_qda = np.zeros(len(train_df))\n",
    "    preds_qda = np.zeros(len(test_df))\n",
    "\n",
    "    oof_svc = np.zeros(len(train_df))\n",
    "    preds_svc = np.zeros(len(test_df))\n",
    "    \n",
    "    oof_knn = np.zeros(len(train_df))\n",
    "    preds_knn = np.zeros(len(test_df))\n",
    "\n",
    "    oof_lr = np.zeros(len(train_df))\n",
    "    preds_lr = np.zeros(len(test_df))\n",
    "    \n",
    "    cols = [c for c in train_df.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n",
    "\n",
    "    for i in tqdm_notebook(range(512)):\n",
    "\n",
    "        # each magic\n",
    "        train = train_df[train_df['wheezy-copper-turtle-magic'] == i]\n",
    "        test = test_df[test_df['wheezy-copper-turtle-magic'] == i]\n",
    "\n",
    "        # for oof\n",
    "        train_idx_origin = train.index\n",
    "        test_idx_origin = test.index\n",
    "\n",
    "\n",
    "        # start point\n",
    "\n",
    "        # new cols\n",
    "        cols = magic_variance_over2[i]\n",
    "\n",
    "        X_train = train.reset_index(drop=True)[cols].values\n",
    "        y_train = train.reset_index(drop=True).target\n",
    "\n",
    "        X_test = test[cols].values\n",
    "\n",
    "        # vstack\n",
    "        data = np.vstack([X_train, X_test])\n",
    "\n",
    "        # PCA\n",
    "        data = KernelPCA(n_components=len(cols), kernel='cosine', random_state=random_state).fit_transform(data)\n",
    "\n",
    "        # Bad\n",
    "        '''\n",
    "        gmm_pred = np.zeros((len(data), 5))\n",
    "        for j in range(5):\n",
    "            gmm = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data)\n",
    "            gmm_pred[:, j] += gmm.predict(data)\n",
    "        '''\n",
    "            \n",
    "        # original\n",
    "        gmm = GMM(n_components=5, random_state=random_state, max_iter=1000, init_params='random').fit(data)\n",
    "        gmm_pred = gmm.predict_proba(data)\n",
    "        gmm_score = gmm.score_samples(data)\n",
    "        gmm_label = gmm.predict(data)\n",
    "        \n",
    "        hist = hist_model(); hist.fit(data)\n",
    "        hist_pred = hist.predict(data).reshape(-1, 1)\n",
    "\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        \n",
    "        # HOXI\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        \n",
    "        # Add Some Features\n",
    "        data = np.hstack([data, gmm_pred])\n",
    "        data = np.hstack([data, hist_pred, gmm_score.reshape(-1, 1)])\n",
    "        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n",
    "        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n",
    "\n",
    "        # STANDARD SCALER\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "\n",
    "        # new train/test\n",
    "        X_train = data[:X_train.shape[0]]\n",
    "        X_test = data[X_train.shape[0]:]\n",
    "\n",
    "        fold = StratifiedKFold(n_splits=5, random_state=random_state)\n",
    "        for tr_idx, val_idx in fold.split(X_train, gmm_label[:X_train.shape[0]]):\n",
    "            \n",
    "            # NuSVC 1\n",
    "            clf = svm.NuSVC(**svnu_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_nusvc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_nusvc[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "\n",
    "            # NuSVC 2\n",
    "            clf = svm.NuSVC(**svnu2_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_nusvc2[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_nusvc2[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "\n",
    "\n",
    "            # qda 3\n",
    "            clf = QuadraticDiscriminantAnalysis(reg_param=0.111)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_qda[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_qda[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "\n",
    "            # SVC 4\n",
    "            clf = svm.SVC(**svc_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_svc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_svc[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "            \n",
    "            # knn 8\n",
    "            clf = KNeighborsClassifier(n_neighbors=16)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_knn[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_knn[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits   \n",
    "            \n",
    "            # LR 5\n",
    "            clf = linear_model.LogisticRegression(**lr_params)\n",
    "            clf.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "            oof_lr[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n",
    "            preds_lr[test_idx_origin] += clf.predict_proba(X_test)[:,1] / fold.n_splits\n",
    "            \n",
    "    oof_train = pd.DataFrame()\n",
    "\n",
    "    oof_train['nusvc'] = oof_nusvc\n",
    "    oof_train['nusvc2'] = oof_nusvc2\n",
    "    oof_train['qda'] = oof_qda\n",
    "    oof_train['svc'] = oof_svc\n",
    "    oof_train['knn'] = oof_knn\n",
    "    oof_train['lr'] = oof_lr\n",
    "    \n",
    "    oof_test = pd.DataFrame()\n",
    "\n",
    "    oof_test['nusvc'] = preds_nusvc\n",
    "    oof_test['nusvc2'] = preds_nusvc2\n",
    "    oof_test['qda'] = preds_qda\n",
    "    oof_test['svc'] = preds_svc\n",
    "    oof_test['knn'] = preds_knn\n",
    "    oof_test['lr'] = preds_lr\n",
    "\n",
    "    print('nusvc', roc_auc_score(train_df['target'], oof_nusvc))\n",
    "    print('nusvc2', roc_auc_score(train_df['target'], oof_nusvc2))\n",
    "    print('qda', roc_auc_score(train_df['target'], oof_qda))\n",
    "    print('svc', roc_auc_score(train_df['target'], oof_svc))\n",
    "    print('knn', roc_auc_score(train_df['target'], oof_knn))\n",
    "    print('knn', roc_auc_score(train_df['target'], oof_lr))\n",
    "    \n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c096b43412b487dba86534abfa55ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nusvc 0.9689702344176699\n",
      "nusvc2 0.9677945162340015\n",
      "qda 0.9715113398462402\n",
      "svc 0.9686216606800445\n",
      "knn 0.9646067212624474\n",
      "knn 0.9411015523640964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63432288bac40c6b6fb023d53fc1fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nusvc 0.9696591804318367\n",
      "nusvc2 0.9686540471963083\n",
      "qda 0.9720590589124234\n",
      "svc 0.9693377923258146\n",
      "knn 0.9655718529047224\n",
      "knn 0.9422620300328199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f114f1f32226482b9aa9d660ca901e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nusvc 0.9619214569945234\n",
      "nusvc2 0.9619334750162918\n",
      "qda 0.9698836662378769\n",
      "svc 0.9612716825321973\n",
      "knn 0.9516456337400276\n",
      "knn 0.9249068731500782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea1471efc5a45ba90939ed03e6bf3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nusvc 0.9629635771024164\n",
      "nusvc2 0.9628122548194054\n",
      "qda 0.9702301221766424\n",
      "svc 0.9623678698555191\n",
      "knn 0.9531371425761838\n",
      "knn 0.9287270671279939\n"
     ]
    }
   ],
   "source": [
    "oof_train_1, oof_test_1 = get_oofs(1)\n",
    "oof_train_2, oof_test_2 = get_oofs(2)\n",
    "oof_train_3, oof_test_3 = get_oofs_2(1)\n",
    "oof_train_4, oof_test_4 = get_oofs_2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble 0.9751324064235614\n"
     ]
    }
   ],
   "source": [
    "x_train_second_layer = oof_train_1 + oof_train_2 + oof_train_3 + oof_train_4\n",
    "x_test_second_layer = oof_test_1 + oof_test_2 + oof_test_3 + oof_test_4\n",
    "print('Ensemble', roc_auc_score(train_df['target'], x_train_second_layer.mean(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 SEED 수정 된 거로 바꿔야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decorator(func):\n",
    "    \n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df = func(*args, **kwargs)\n",
    "        \n",
    "        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n",
    "        print(\"TotalTime: \", time.time() - start_time)\n",
    "        return df\n",
    "        \n",
    "    return wrapper\n",
    "\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, params=None, **kwargs):\n",
    "        \"\"\"\n",
    "        params['random_state'] = kwargs.get('seed', 0)\n",
    "        self.clf = clf(**params)\n",
    "        self.is_classification_problem = True\n",
    "        \"\"\"\n",
    "        if 'seed' in kwargs:\n",
    "            params['random_state'] = kwargs.get('seed', 0)\n",
    "        self.clf = clf(**params)\n",
    "        self.is_classification_problem = True\n",
    "    #@time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        if len(np.unique(y_train)) > 30:\n",
    "            self.is_classification_problem = False\n",
    "            \n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_classification_problem is True:\n",
    "            return self.clf.predict_proba(x)[:,1]\n",
    "        else:\n",
    "            return self.clf.predict(x)\n",
    "    \n",
    "class LgbmWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        if 'seed' in kwargs:\n",
    "            self.param['seed'] = kwargs.get('seed', 0)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "        self.feature_importance = pd.DataFrame()\n",
    "        \n",
    "    #@time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "                \n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n",
    "            del dtrain   \n",
    "        else:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n",
    "            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n",
    "                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n",
    "                                  verbose_eval=self.verbose_eval)\n",
    "            \n",
    "            try:\n",
    "                self.feature_importance = pd.DataFrame()\n",
    "                self.feature_importance[\"Feature\"] = x_train.columns\n",
    "                self.feature_importance[\"Importance\"] = self.clf.feature_importance()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "            del dtrain, dvalid\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def get_importance_df(self):\n",
    "        return self.feature_importance\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n",
    "    \n",
    "    def plot_importance(self):\n",
    "        lgb.plot_importance(self.clf, max_num_features=50, height=0.7, figsize=(10,30))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@time_decorator\n",
    "def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n",
    "    nfolds = kwargs.get('NFOLDS', 5)\n",
    "    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n",
    "    kfold_random_state = kwargs.get('kfold_random_state', 0)\n",
    "    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n",
    "    inner_predict = kwargs.get('inner_predict', True)\n",
    "    export_feature_importance = kwargs.get('export_feature_importance', True)\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    \n",
    "    kf_split = None\n",
    "    if stratified_kfold_ytrain is None:\n",
    "        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "        kf_split = kf.split(x_train)\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n",
    "        \n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "\n",
    "    cv_sum = 0\n",
    "    \n",
    "    # before running model, print model param\n",
    "    # lightgbm model and xgboost model use get_params()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if clf.clf is not None:\n",
    "            print(clf.clf)\n",
    "    except:\n",
    "        print(clf)\n",
    "        print(clf.get_params())\n",
    "    \"\"\"\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, (train_index, cross_index) in enumerate(kf_split):\n",
    "        x_tr, x_cr = None, None\n",
    "        y_tr, y_cr = None, None\n",
    "        if isinstance(x_train, pd.DataFrame):\n",
    "            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n",
    "            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n",
    "        else:\n",
    "            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n",
    "            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_cr, y_cr)\n",
    "        \n",
    "        if isinstance(clf, LgbmWrapper) is True:\n",
    "            feature_importance_df = pd.concat([feature_importance_df, clf.get_importance_df()], axis=0)\n",
    "    \n",
    "        oof_train[cross_index] = clf.predict(x_cr)\n",
    "        if inner_predict is True:\n",
    "            oof_test += clf.predict(x_test)\n",
    "        \n",
    "        cv_score = eval_func(y_cr, oof_train[cross_index])\n",
    "        \n",
    "        #print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n",
    "        cv_sum = cv_sum + cv_score\n",
    "        \n",
    "        del x_tr, x_cr, y_tr, y_cr\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    score = cv_sum / nfolds\n",
    "    #print(\"Average CV-Score: \", score)\n",
    "    #print(\"OOF CV-Score: \", eval_func(y_train, oof_train))\n",
    "    \n",
    "    if export_feature_importance is True:\n",
    "        print(\"Export Feature Importance\")\n",
    "        filename = '{}_cv{:.6f}'.format(datetime.now().strftime('%Y%m%d_%H%M%S'), score)\n",
    "        if os.path.isdir(\"importance/\") is True:\n",
    "            feature_importance_df.to_csv('importance/importance_{}.csv'.format(filename),index=False)\n",
    "        else:\n",
    "            feature_importance_df.to_csv('importance_{}.csv'.format(filename),index=False)\n",
    "            \n",
    "    if inner_predict is True:\n",
    "        oof_test = oof_test/nfolds\n",
    "    else:\n",
    "        # Using All Dataset, retrain\n",
    "        clf.train(x_train, y_train)\n",
    "        oof_test = clf.predict(x_test)\n",
    "\n",
    "    return oof_train, oof_test, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_second_layer1 = pd.DataFrame(x_train_second_layer)\n",
    "x_test_second_layer1 = pd.DataFrame(x_test_second_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "import warnings\n",
    "import itertools\n",
    "from functools import wraps\n",
    "import functools\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, neighbors, linear_model\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.mixture import GaussianMixture as GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97489\tvalid_1's auc: 0.975421\n",
      "[200]\ttraining's auc: 0.976337\tvalid_1's auc: 0.975243\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's auc: 0.975346\tvalid_1's auc: 0.97561\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.974773\tvalid_1's auc: 0.975829\n",
      "[200]\ttraining's auc: 0.976176\tvalid_1's auc: 0.976246\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.975965\tvalid_1's auc: 0.976367\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97522\tvalid_1's auc: 0.974469\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's auc: 0.975101\tvalid_1's auc: 0.974586\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975166\tvalid_1's auc: 0.97424\n",
      "[200]\ttraining's auc: 0.976905\tvalid_1's auc: 0.974521\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.975917\tvalid_1's auc: 0.974799\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975149\tvalid_1's auc: 0.974484\n",
      "[200]\ttraining's auc: 0.976677\tvalid_1's auc: 0.974668\n",
      "[300]\ttraining's auc: 0.978157\tvalid_1's auc: 0.974821\n",
      "[400]\ttraining's auc: 0.978742\tvalid_1's auc: 0.974752\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttraining's auc: 0.978341\tvalid_1's auc: 0.974847\n",
      "Export Feature Importance\n",
      "Export Feature Importance\n",
      "0.9752418213147422\n",
      "0.9749419357482925\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97489\tvalid_1's auc: 0.975421\n",
      "[200]\ttraining's auc: 0.976337\tvalid_1's auc: 0.975243\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's auc: 0.975346\tvalid_1's auc: 0.97561\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.974773\tvalid_1's auc: 0.975829\n",
      "[200]\ttraining's auc: 0.976176\tvalid_1's auc: 0.976246\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.975965\tvalid_1's auc: 0.976367\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97522\tvalid_1's auc: 0.974469\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's auc: 0.975101\tvalid_1's auc: 0.974586\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975166\tvalid_1's auc: 0.97424\n",
      "[200]\ttraining's auc: 0.976905\tvalid_1's auc: 0.974521\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.975917\tvalid_1's auc: 0.974799\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975149\tvalid_1's auc: 0.974484\n",
      "[200]\ttraining's auc: 0.976677\tvalid_1's auc: 0.974668\n",
      "[300]\ttraining's auc: 0.978157\tvalid_1's auc: 0.974821\n",
      "[400]\ttraining's auc: 0.978742\tvalid_1's auc: 0.974752\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttraining's auc: 0.978341\tvalid_1's auc: 0.974847\n",
      "Export Feature Importance\n",
      "Export Feature Importance\n",
      "0.9752418213147422\n",
      "0.9749419357482925\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97489\tvalid_1's auc: 0.975421\n",
      "[200]\ttraining's auc: 0.976337\tvalid_1's auc: 0.975243\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's auc: 0.975346\tvalid_1's auc: 0.97561\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.974773\tvalid_1's auc: 0.975829\n",
      "[200]\ttraining's auc: 0.976176\tvalid_1's auc: 0.976246\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.975965\tvalid_1's auc: 0.976367\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97522\tvalid_1's auc: 0.974469\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's auc: 0.975101\tvalid_1's auc: 0.974586\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975166\tvalid_1's auc: 0.97424\n",
      "[200]\ttraining's auc: 0.976905\tvalid_1's auc: 0.974521\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.975917\tvalid_1's auc: 0.974799\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975149\tvalid_1's auc: 0.974484\n",
      "[200]\ttraining's auc: 0.976677\tvalid_1's auc: 0.974668\n",
      "[300]\ttraining's auc: 0.978157\tvalid_1's auc: 0.974821\n",
      "[400]\ttraining's auc: 0.978742\tvalid_1's auc: 0.974752\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttraining's auc: 0.978341\tvalid_1's auc: 0.974847\n",
      "Export Feature Importance\n",
      "Export Feature Importance\n",
      "0.9752418213147422\n",
      "0.9749419357482925\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97489\tvalid_1's auc: 0.975421\n",
      "[200]\ttraining's auc: 0.976337\tvalid_1's auc: 0.975243\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's auc: 0.975346\tvalid_1's auc: 0.97561\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.974773\tvalid_1's auc: 0.975829\n",
      "[200]\ttraining's auc: 0.976176\tvalid_1's auc: 0.976246\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.975965\tvalid_1's auc: 0.976367\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.97522\tvalid_1's auc: 0.974469\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's auc: 0.975101\tvalid_1's auc: 0.974586\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975166\tvalid_1's auc: 0.97424\n",
      "[200]\ttraining's auc: 0.976905\tvalid_1's auc: 0.974521\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.975917\tvalid_1's auc: 0.974799\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.975149\tvalid_1's auc: 0.974484\n",
      "[200]\ttraining's auc: 0.976677\tvalid_1's auc: 0.974668\n",
      "[300]\ttraining's auc: 0.978157\tvalid_1's auc: 0.974821\n",
      "[400]\ttraining's auc: 0.978742\tvalid_1's auc: 0.974752\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttraining's auc: 0.978341\tvalid_1's auc: 0.974847\n",
      "Export Feature Importance\n",
      "Export Feature Importance\n",
      "0.9752418213147422\n",
      "0.9749419357482925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "        #'bagging_freq': 5,\n",
    "        #'bagging_fraction': 0.8,\n",
    "        'min_child_weight':6.790,\n",
    "        \"subsample_for_bin\":50000,\n",
    "        'bagging_seed': 0,\n",
    "        'boost_from_average':'true',\n",
    "        'boost': 'gbdt',\n",
    "        'feature_fraction': 0.450,\n",
    "        'bagging_fraction': 0.343,\n",
    "        'learning_rate': 0.025,\n",
    "        'max_depth': 10,\n",
    "        'metric':'auc',\n",
    "        'min_data_in_leaf': 78,\n",
    "        'min_sum_hessian_in_leaf': 8, \n",
    "        'num_leaves': 18,\n",
    "        'num_threads': 8,\n",
    "        'tree_learner': 'serial',\n",
    "        'objective': 'binary', \n",
    "        'verbosity': 1,\n",
    "        'lambda_l1': 7.961,\n",
    "        'lambda_l2': 7.781\n",
    "        #'reg_lambda': 0.3,\n",
    "    }\n",
    "\n",
    "mlp16_params = {'activation':'relu','solver':'lbfgs','tol':1e-06, 'hidden_layer_sizes':(16, ), 'random_state':42}\n",
    "\n",
    "lgbm_meta_model = LgbmWrapper(params=param, num_rounds = 2000, ealry_stopping=100)\n",
    "mlp_meta_model = SklearnWrapper(neural_network.MLPClassifier,mlp16_params)\n",
    "\n",
    "third_number = 4\n",
    "oof_train_5 = pd.DataFrame()\n",
    "oof_test_5 = pd.DataFrame()\n",
    "\n",
    "third_oof = np.zeros(len(train_df))\n",
    "third_pred = np.zeros(len(test_df))\n",
    "third_oof1 = np.zeros(len(train_df))\n",
    "third_pred1 = np.zeros(len(test_df))\n",
    "\n",
    "for SEED in np.arange(third_number):\n",
    "    second_oof, second_pred, second_score = get_oof(lgbm_meta_model, x_train_second_layer1, train_df['target'], x_test_second_layer1, eval_func=roc_auc_score, NFOLDS=5, kfold_random_sate= SEED )\n",
    "    second_oof1, second_pred1, second_score1 = get_oof(mlp_meta_model, x_train_second_layer1, train_df['target'], x_test_second_layer1, eval_func=roc_auc_score, NFOLDS=5, kfold_random_sate= SEED )\n",
    "\n",
    "    third_oof += second_oof\n",
    "    third_pred += second_pred\n",
    "    print(second_score)\n",
    "    third_oof1 += second_oof1\n",
    "    third_pred1 += second_pred1\n",
    "    print(second_score1)\n",
    "    print(\"\")\n",
    "    \n",
    "oof_train_5['lgb'] = third_oof\n",
    "oof_test_5['lgb'] = third_pred\n",
    "oof_train_5['mlp'] = third_oof1\n",
    "oof_test_5['mlp'] = third_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble 0.9751519056446724\n"
     ]
    }
   ],
   "source": [
    "x_train_second_layer = oof_train_1 + oof_train_2 + oof_train_3 + oof_train_4  \n",
    "x_test_second_layer = oof_test_1 + oof_test_2 + oof_test_3 + oof_test_4 \n",
    "\n",
    "x_train_second_layer = pd.concat([x_train_second_layer,oof_train_5],axis=1)\n",
    "x_test_second_layer = pd.concat([x_test_second_layer,oof_test_5],axis=1)\n",
    "                                     \n",
    "print('Ensemble', roc_auc_score(train_df['target'], x_train_second_layer.mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "submit[\"target\"] = x_test_second_layer.mean(1)\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
