{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from datetime import datetime, timedelta,date\n",
    "import time\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection,johnson_lindenstrauss_min_dim\n",
    "from sklearn.decomposition import PCA, FastICA,NMF,LatentDirichletAllocation,IncrementalPCA,MiniBatchSparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD,FactorAnalysis,KernelPCA\n",
    "\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from functools import wraps\n",
    "import functools\n",
    "#settings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Utility Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix(group_col, target_col, prefix=None):\n",
    "    if isinstance(group_col, list) is True:\n",
    "        g = '_'.join(group_col)\n",
    "    else:\n",
    "        g = group_col\n",
    "    if isinstance(target_col, list) is True:\n",
    "        t = '_'.join(target_col)\n",
    "    else:\n",
    "        t = target_col\n",
    "    if prefix is not None:\n",
    "        return prefix + '_' + g + '_' + t\n",
    "    return g + '_' + t\n",
    "    \n",
    "def groupby_helper(df, group_col, target_col, agg_method, prefix_param=None):\n",
    "    try:\n",
    "        prefix = get_prefix(group_col, target_col, prefix_param)\n",
    "        print(group_col, target_col, agg_method)\n",
    "        group_df = df.groupby(group_col)[target_col].agg(agg_method)\n",
    "        group_df.columns = ['{}_{}'.format(prefix, m) for m in agg_method]\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "    return group_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decorator(func):\n",
    "    \n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df = func(*args, **kwargs)\n",
    "        \n",
    "        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n",
    "        print(\"TotalTime: \", time.time() - start_time)\n",
    "        return df\n",
    "        \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, params=None, **kwargs):\n",
    "        params['random_state'] = kwargs.get('seed', 0)\n",
    "        self.clf = clf(**params)\n",
    "        self.is_classification_problem = True\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        if len(np.unique(y_train)) > 30:\n",
    "            self.is_classification_problem = False\n",
    "            \n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_classification_problem is True:\n",
    "            return self.clf.predict_proba(x)[:,1]\n",
    "        else:\n",
    "            return self.clf.predict(x)\n",
    "    \n",
    "\n",
    "class CatboostWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        try:\n",
    "            if params is None:\n",
    "                raise(\"Parameter를 입력하세요!!\")\n",
    "            self.param = params\n",
    "            seed = kwargs.get('seed', None)\n",
    "            \n",
    "            if seed is not None:\n",
    "                self.param['random_seed'] = seed\n",
    "                \n",
    "            num_rounds = kwargs.get('num_rounds', None)\n",
    "            if num_rounds is not None:\n",
    "                self.param['num_boost_round'] = num_rounds\n",
    "            \n",
    "            early_stopping = kwargs.get('ealry_stopping', None)\n",
    "            if early_stopping is not None:\n",
    "                self.param['early_stopping_rounds'] = early_stopping\n",
    "            \n",
    "            eval_function = kwargs.get('eval_function', None)\n",
    "            if eval_function is not None:\n",
    "                self.param['eval_metric'] = eval_function\n",
    "            \n",
    "            verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "            if verbose_eval is not None:\n",
    "                self.param['verbose'] = verbose_eval\n",
    "                \n",
    "            self.best_round = 0\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "         \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            train_round = self.clf.tree_count_\n",
    "            if self.best_round > 0:\n",
    "                train_round = self.best_round\n",
    "            \n",
    "            self.param['iterations'] = train_round\n",
    "            self.clf = cb.CatBoostRegressor(**self.param)\n",
    "            self.clf.fit(x_train, y_train, use_best_model=True)\n",
    "        else:\n",
    "            self.clf = cb.CatBoostRegressor(**self.param)\n",
    "            self.clf.fit(x_train, y_train,\n",
    "                         eval_set=[(x_train, y_train),(x_cross, y_cross)],\n",
    "                         use_best_model=True)\n",
    "            self.best_round = max(self.best_round, self.clf.tree_count_)\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "    \n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.param['seed'] = kwargs.get('seed', 0)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "    \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        need_cross_validation = True\n",
    "       \n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "                \n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "            \n",
    "            print(train_round)\n",
    "            self.clf = xgb.train(self.param, dtrain, train_round)\n",
    "            del dtrain\n",
    "        else:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)\n",
    "            dvalid = xgb.DMatrix(x_cross, label=y_cross, silent=True)\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "            self.clf = xgb.train(self.param, dtrain, self.num_rounds, watchlist, feval=self.eval_function,\n",
    "                                 early_stopping_rounds=self.early_stopping,\n",
    "                                 verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(xgb.DMatrix(x), ntree_limit=self.best_round)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "    \n",
    "    \n",
    "class LgbmWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.param['seed'] = kwargs.get('seed', 0)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "        \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "                \n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n",
    "            del dtrain   \n",
    "        else:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n",
    "            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n",
    "                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n",
    "                                  verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "            del dtrain, dvalid\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n",
    "    \n",
    "    def plot_importance(self):\n",
    "        lgb.plot_importance(self.clf, max_num_features=50, height=0.7, figsize=(10,30))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n",
    "    nfolds = kwargs.get('NFOLDS', 5)\n",
    "    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n",
    "    kfold_random_state = kwargs.get('kfold_random_state', 0)\n",
    "    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "\n",
    "    cv_sum = 0\n",
    "    \n",
    "    if stratified_kfold_ytrain is None:\n",
    "        stratified_kfold_ytrain = y_train\n",
    "    \n",
    "    # before running model, print model param\n",
    "    # lightgbm model and xgboost model use get_params()\n",
    "    try:\n",
    "        if clf.clf is not None:\n",
    "            print(clf.clf)\n",
    "    except:\n",
    "        print(clf)\n",
    "        print(clf.get_params())\n",
    "\n",
    "    for i, (train_index, cross_index) in enumerate(kf.split(x_train, stratified_kfold_ytrain)):\n",
    "        x_tr, x_cr = None, None\n",
    "        y_tr, y_cr = None, None\n",
    "        if isinstance(x_train, pd.DataFrame):\n",
    "            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n",
    "            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n",
    "        else:\n",
    "            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n",
    "            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_cr, y_cr)\n",
    "        \n",
    "        oof_train[cross_index] = clf.predict(x_cr)\n",
    "\n",
    "        cv_score = eval_func(y_cr, oof_train[cross_index])\n",
    "        \n",
    "        print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n",
    "        cv_sum = cv_sum + cv_score\n",
    "        \n",
    "        del x_tr, x_cr, y_tr, y_cr\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    score = cv_sum / nfolds\n",
    "    print(\"Average CV-Score: \", score)\n",
    "\n",
    "    # Using All Dataset, retrain\n",
    "    clf.train(x_train, y_train)\n",
    "    oof_test = clf.predict(x_test)\n",
    "\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def kfold_test(clf, x_train, y_train, eval_func, **kwargs):\n",
    "    nfolds = kwargs.get('NFOLDS', 5)\n",
    "    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n",
    "    kfold_random_state = kwargs.get('kfold_random_sate', 0)\n",
    "\n",
    "    ntrain = x_train.shape[0]\n",
    "\n",
    "    kf = KFold(ntrain, n_folds=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "    \n",
    "    cv_sum = 0\n",
    "    try:\n",
    "        if clf.clf is not None:\n",
    "            print(clf.clf)\n",
    "    except:\n",
    "        print(clf)\n",
    "        print(clf.get_params())\n",
    "\n",
    "    best_rounds = []\n",
    "    for i, (train_index, cross_index) in enumerate(kf):\n",
    "        x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n",
    "        y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_cr, y_cr)\n",
    "\n",
    "        cv_score = eval_func(y_cr, clf.predict(x_cr))\n",
    "\n",
    "        print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n",
    "        cv_sum = cv_sum + cv_score\n",
    "        best_rounds.append(clf.clf.best_iteration)\n",
    "\n",
    "    score = cv_sum / nfolds\n",
    "    print(\"Average CV-Score: \", score)\n",
    "\n",
    "    return score, np.max(best_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* LGBM Deep, Middle, Shallow\n",
    ">* LGBM Dart\n",
    ">* Catboost, Xgboost Deep, Middle\n",
    ">* Random Forest, ExtraTree\n",
    ">* Lasso, Ridge\n",
    ">* NN Deep, Middle, Shallow -> 이건 아직<br>\n",
    "오늘은 위에것만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('input/train_v1.csv')\n",
    "test_df = pd.read_csv('input/test_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dayofyear'] outliers ['mean']\n",
      "['elapsed_time'] outliers ['mean']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'card_id_card_id_month_lag_purchase_amount_count_sum',\n",
       " 'card_id_card_id_month_lag_purchase_amount_count_mean',\n",
       " 'card_id_card_id_month_lag_purchase_amount_count_std',\n",
       " 'card_id_card_id_month_lag_purchase_amount_mean_sum',\n",
       " 'card_id_card_id_month_lag_purchase_amount_mean_mean',\n",
       " 'card_id_card_id_month_lag_purchase_amount_mean_std',\n",
       " 'auth_0_card_id_month_nunique',\n",
       " 'auth_0_card_id_month_max',\n",
       " 'auth_0_card_id_month_min',\n",
       " 'auth_0_card_id_month_mean',\n",
       " 'auth_0_card_id_month_std',\n",
       " 'auth_1_card_id_month_nunique',\n",
       " 'auth_1_card_id_month_max',\n",
       " 'auth_1_card_id_month_min',\n",
       " 'auth_1_card_id_month_mean',\n",
       " 'auth_1_card_id_month_std',\n",
       " 'new_hist_card_id_month_nunique',\n",
       " 'new_hist_card_id_month_max',\n",
       " 'new_hist_card_id_month_min',\n",
       " 'new_hist_card_id_month_mean',\n",
       " 'card_id_merchant_id_nunique',\n",
       " 'auth_0_card_id_merchant_category_id_nunique',\n",
       " 'auth_1_card_id_merchant_category_id_nunique',\n",
       " 'auth_0_card_id_subsector_id_nunique',\n",
       " 'auth_1_card_id_subsector_id_nunique',\n",
       " 'auth_0_card_id_state_id_nunique',\n",
       " 'auth_1_card_id_state_id_nunique',\n",
       " 'card_id_city_id_nunique',\n",
       " 'new_hist_card_id_merchant_category_id_nunique',\n",
       " 'new_hist_card_id_subsector_id_nunique',\n",
       " 'new_hist_card_id_state_id_nunique',\n",
       " 'hist_hour_nunique',\n",
       " 'hist_hour_mean',\n",
       " 'hist_hour_min',\n",
       " 'hist_hour_max',\n",
       " 'hist_weekofyear_nunique',\n",
       " 'hist_weekofyear_mean',\n",
       " 'hist_weekofyear_min',\n",
       " 'hist_weekofyear_max',\n",
       " 'hist_dayofweek_nunique',\n",
       " 'hist_dayofweek_mean',\n",
       " 'hist_dayofweek_min',\n",
       " 'hist_dayofweek_max',\n",
       " 'hist_year_nunique',\n",
       " 'hist_year_mean',\n",
       " 'hist_year_max',\n",
       " 'hist_purchase_amount_sum',\n",
       " 'hist_purchase_amount_max',\n",
       " 'hist_purchase_amount_min',\n",
       " 'hist_purchase_amount_mean',\n",
       " 'hist_purchase_amount_var',\n",
       " 'hist_installments_sum',\n",
       " 'hist_installments_max',\n",
       " 'hist_installments_min',\n",
       " 'hist_installments_mean',\n",
       " 'hist_installments_var',\n",
       " 'hist_purchase_date_max',\n",
       " 'hist_purchase_date_min',\n",
       " 'hist_month_lag_max',\n",
       " 'hist_month_lag_min',\n",
       " 'hist_month_lag_mean',\n",
       " 'hist_month_lag_var',\n",
       " 'hist_month_diff_mean',\n",
       " 'hist_month_diff_max',\n",
       " 'hist_month_diff_min',\n",
       " 'hist_month_diff_var',\n",
       " 'hist_weekend_sum',\n",
       " 'hist_weekend_mean',\n",
       " 'hist_weekend_min',\n",
       " 'hist_weekend_max',\n",
       " 'hist_category_1_sum',\n",
       " 'hist_category_1_mean',\n",
       " 'hist_category_1_min',\n",
       " 'hist_category_1_max',\n",
       " 'hist_authorized_flag_sum',\n",
       " 'hist_authorized_flag_mean',\n",
       " 'hist_authorized_flag_min',\n",
       " 'hist_card_id_size',\n",
       " 'hist_reference_date_median',\n",
       " 'hist_purchase_date_diff',\n",
       " 'hist_purchase_date_average',\n",
       " 'hist_purchase_date_uptonow',\n",
       " 'hist_purchase_date_uptomin',\n",
       " 'new_hist_hour_nunique',\n",
       " 'new_hist_hour_mean',\n",
       " 'new_hist_hour_min',\n",
       " 'new_hist_hour_max',\n",
       " 'new_hist_weekofyear_nunique',\n",
       " 'new_hist_weekofyear_mean',\n",
       " 'new_hist_weekofyear_min',\n",
       " 'new_hist_weekofyear_max',\n",
       " 'new_hist_dayofweek_nunique',\n",
       " 'new_hist_dayofweek_mean',\n",
       " 'new_hist_dayofweek_min',\n",
       " 'new_hist_dayofweek_max',\n",
       " 'new_hist_year_nunique',\n",
       " 'new_hist_year_mean',\n",
       " 'new_hist_year_min',\n",
       " 'new_hist_year_max',\n",
       " 'new_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_var',\n",
       " 'new_hist_installments_sum',\n",
       " 'new_hist_installments_max',\n",
       " 'new_hist_installments_min',\n",
       " 'new_hist_installments_mean',\n",
       " 'new_hist_installments_var',\n",
       " 'new_hist_purchase_date_max',\n",
       " 'new_hist_purchase_date_min',\n",
       " 'new_hist_month_lag_max',\n",
       " 'new_hist_month_lag_min',\n",
       " 'new_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_var',\n",
       " 'new_hist_month_diff_mean',\n",
       " 'new_hist_month_diff_max',\n",
       " 'new_hist_month_diff_min',\n",
       " 'new_hist_month_diff_var',\n",
       " 'new_hist_weekend_sum',\n",
       " 'new_hist_weekend_mean',\n",
       " 'new_hist_weekend_min',\n",
       " 'new_hist_weekend_max',\n",
       " 'new_hist_category_1_sum',\n",
       " 'new_hist_category_1_mean',\n",
       " 'new_hist_category_1_min',\n",
       " 'new_hist_category_1_max',\n",
       " 'new_hist_card_id_size',\n",
       " 'new_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptomin',\n",
       " 'dayofweek',\n",
       " 'weekofyear',\n",
       " 'dayofyear',\n",
       " 'quarter',\n",
       " 'month',\n",
       " 'elapsed_time',\n",
       " 'hist_first_buy',\n",
       " 'hist_last_buy',\n",
       " 'new_hist_first_buy',\n",
       " 'new_hist_last_buy',\n",
       " 'hist_diff_reference_date_first',\n",
       " 'hist_diff_reference_date_last',\n",
       " 'new_hist_diff_reference_date_first',\n",
       " 'new_hist_diff_reference_date_last',\n",
       " 'hist_diff_first_last',\n",
       " 'new_hist_diff_first_last',\n",
       " 'diff_new_hist_date_min_max',\n",
       " 'diff_new_hist_date_max_max',\n",
       " 'hist_flag_ratio',\n",
       " 'card_id_total',\n",
       " 'purchase_amount_total',\n",
       " 'new_hist_flag_ratio',\n",
       " 'dayofyear_outliers_mean',\n",
       " 'elapsed_time_outliers_mean']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df = all_df.loc[all_df['target'].notnull()]\n",
    "#test_df = all_df.loc[all_df['target'].isnull()]\n",
    "\n",
    "train_df['outliers'] = 0\n",
    "train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "train_df['outliers'].value_counts()\n",
    "\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = train_df.groupby([f])['outliers'].mean()\n",
    "    train_df[f] = train_df[f].map(order_label)\n",
    "    test_df[f] = test_df[f].map(order_label)\n",
    "\n",
    "group_df = groupby_helper(train_df,['dayofyear'], 'outliers',['mean'])\n",
    "train_df = train_df.merge(group_df, on=['dayofyear'], how='left')\n",
    "test_df = test_df.merge(group_df, on=['dayofyear'], how='left')\n",
    "\n",
    "group_df = groupby_helper(train_df,['elapsed_time'], 'outliers',['mean'])\n",
    "train_df = train_df.merge(group_df, on=['elapsed_time'], how='left')\n",
    "test_df = test_df.merge(group_df, on=['elapsed_time'], how='left')\n",
    "\n",
    "train_columns = [c for c in train_df.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "train_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.copy()\n",
    "x_test = test_df.copy()\n",
    "y_train = train_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreeRegressior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_parmas = {\n",
    "    'criterion':'mse', 'max_leaf_nodes':-1, 'n_estimators':800, 'min_impurity_split':0.0000001,\n",
    "    'max_features':0.6, 'max_depth':9, 'min_samples_leaf':30, 'min_samples_split':2,\n",
    "    'min_weight_fraction_leaf':0.0, 'bootstrap':True,'n_jobs':-1,'warm_start':False,\n",
    "    'random_state':6, 'verbose':True\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'criterion':'mse', 'max_leaf_nodes':-1, 'n_estimators':500, 'min_impurity_split':0.0000001,\n",
    "    'max_features':0.6, 'max_depth':9, 'min_samples_leaf':30, 'min_samples_split':2,\n",
    "    'min_weight_fraction_leaf':0.0, 'bootstrap':True,'n_jobs':-1,\n",
    "    'random_state':6, 'verbose':True\n",
    "}\n",
    "\n",
    "lasso_params={\n",
    "    'alpha':0.003,\n",
    "    'normalize':True,\n",
    "    'max_iter':200,'fit_intercept':True,'tol':0.007,\n",
    "    'warm_start':True\n",
    "}\n",
    "\n",
    "ridge_params={\n",
    "    'alpha':0.2,\n",
    "    'normalize':True,\n",
    "    'max_iter':200,'fit_intercept':False,'solver':'auto'\n",
    "}\n",
    "\n",
    "lgbm_param1 = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.015,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 24,\n",
    "         \"seed\": 6}\n",
    "\n",
    "lgbm_param2 = {'num_leaves': 22,\n",
    "         'min_data_in_leaf': 70, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 7,\n",
    "         'learning_rate': 0.015,\n",
    "         \"min_child_samples\": 50,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"subsample\": 0.6,\n",
    "         \"colsample\": 0.8,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 24,\n",
    "         \"seed\": 6}\n",
    "\n",
    "lgbm_param3 = {'num_leaves': 10,\n",
    "         'min_data_in_leaf': 10, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 5,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 10,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"subsample\": 0.8,\n",
    "         \"colsample\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 24,\n",
    "         \"seed\": 6}\n",
    "\n",
    "lgbm_param4 = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.02,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"dart\",\n",
    "         'drop_rate':0.3, \n",
    "         'skip_drop':0.4,\n",
    "         'max_drop':50,\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 24,\n",
    "         \"seed\": 6}\n",
    "\n",
    "xgb_params1 = {\n",
    "    'booster':'gbtree', 'objective':'reg:linear', 'max_leaves':0, 'eta':0.02, 'gamma':1,\n",
    "    'max_depth':4, 'colsample_bylevel':1.0, 'min_child_weight':4.0, 'max_delta_step':0.0, 'subsample':0.8, \n",
    "    'colsample_bytree':0.5,'alpha':1.0, 'lambda':5.0, 'seed':6,'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "cat_param = {'iterations':3000,'rsm':0.8,'depth':5, 'learning_rate':0.037, 'bootstrap_type':'Bernoulli',\n",
    "             'eval_metric':'RMSE','l2_leaf_reg':40,'od_wait':100,'loss_function':'RMSE','od_type':'Iter',\n",
    "            'random_seed':6,'subsample':0.7,'verbose':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_model = SklearnWrapper(clf = ExtraTreesRegressor,params=et_parmas)\n",
    "rf_model = SklearnWrapper(clf = RandomForestRegressor,params=rf_params)\n",
    "\n",
    "lgbm_model1 = LgbmWrapper(params=lgbm_param1, num_rounds = 10000, ealry_stopping=100,\n",
    "                                   verbose_eval=100, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "lgbm_model2 = LgbmWrapper(params=lgbm_param2, num_rounds = 10000, ealry_stopping=100,\n",
    "                                   verbose_eval=100, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "lgbm_model3 = LgbmWrapper(params=lgbm_param3, num_rounds = 10000, ealry_stopping=100,\n",
    "                                   verbose_eval=100, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "lgbm_model4 = LgbmWrapper(params=lgbm_param4, num_rounds = 10000, ealry_stopping=100,\n",
    "                                   verbose_eval=100, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "xgb_model1 = XgbWrapper(params=xgb_params1, num_rounds = 10000, ealry_stopping=100,\n",
    "                                   verbose_eval=100, base_score=True, maximize=False,\n",
    "                                   y_value_log=False)\n",
    "\n",
    "cat_model = CatboostWrapper(params=cat_param,verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StartTime:  2019-01-25 01:29:53.066259\n",
      "ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=9,\n",
      "          max_features=0.6, max_leaf_nodes=-1, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=1e-07, min_samples_leaf=30,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=800, n_jobs=-1, oob_score=False, random_state=0,\n",
      "          verbose=True, warm_start=False)\n",
      "\n",
      "StartTime:  2019-01-25 01:29:53.315035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2019-01-25 01:31:04.011213\n",
      "TotalTime:  70.69621539115906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=24)]: Done 800 out of 800 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 /  CV-Score: 3.677008\n",
      "\n",
      "StartTime:  2019-01-25 01:31:04.628566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2019-01-25 01:32:16.043426\n",
      "TotalTime:  71.41486382484436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=24)]: Done 800 out of 800 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 /  CV-Score: 3.692573\n",
      "\n",
      "StartTime:  2019-01-25 01:32:16.640145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2019-01-25 01:33:28.456097\n",
      "TotalTime:  71.81598258018494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=24)]: Done 800 out of 800 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 /  CV-Score: 3.682249\n",
      "\n",
      "StartTime:  2019-01-25 01:33:29.050039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2019-01-25 01:34:40.666760\n",
      "TotalTime:  71.61674880981445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=24)]: Done 800 out of 800 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 /  CV-Score: 3.660616\n",
      "\n",
      "StartTime:  2019-01-25 01:34:41.261594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   35.6s\n"
     ]
    }
   ],
   "source": [
    "et_train, et_test, et_cv_score = get_oof(et_model, x_train[train_columns].fillna(-1), y_train, x_test[train_columns].fillna(-1), \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "\n",
    "rf_train, rf_test, rf_cv_score = get_oof(rf_model, x_train[train_columns].fillna(-1), y_train, x_test[train_columns].fillna(-1), \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "\n",
    "lgbm1_train, lgbm1_test, lgbm1_cv_score = get_oof(lgbm_model1, x_train[train_columns], y_train, x_test[train_columns], \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "lgbm2_train, lgbm2_test, lgbm2_cv_score = get_oof(lgbm_model2, x_train[train_columns], y_train, x_test[train_columns], \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "lgbm3_train, lgbm3_test, lgbm3_cv_score = get_oof(lgbm_model3, x_train[train_columns], y_train, x_test[train_columns], \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "lgbm4_train, lgbm4_test, lgbm4_cv_score = get_oof(lgbm_model4, x_train[train_columns], y_train, x_test[train_columns], \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "\n",
    "xgb_train, xgb_test, xgb_cv_score = get_oof(xgb_model1, x_train[train_columns], y_train, x_test[train_columns], \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)\n",
    "\n",
    "cat_train, cat_test, cat_cv_score = get_oof(cat_model, x_train[train_columns], y_train, x_test[train_columns], \n",
    "                            rmse, NFOLDS=9, kfold_random_state=4950, stratifed_kfold_y_value=x_train['outliers'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_second_layer = np.concatenate((et_train, rf_train, lgbm1_train,\n",
    "                         lgbm2_train, lgbm3_train, lgbm4_train,\n",
    "                         xgb_train,cat_train), axis=1)\n",
    "\n",
    "x_test_second_layer = np.concatenate((et_test, rf_test, lgbm1_test,\n",
    "                         lgbm2_test, lgbm3_test, lgbm4_test,\n",
    "                         xgb_test,cat_test), axis=1)\n",
    "x_train = pd.DataFrame(x_train_second_layer)\n",
    "x_test = pd.DataFrame(x_test_second_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_ex_no = 31\n",
    "lgbm_meta_params = {\n",
    "    'boosting':'gbdt', 'num_leaves':28, 'learning_rate':0.03, 'min_sum_hessian_in_leaf':0.1,\n",
    "    'max_depth':7, 'feature_fraction':0.6, 'min_data_in_leaf':70, 'poission_max_delta_step':0.7,\n",
    "    'bagging_fraction':0.8, 'min_gain_to_split':0, 'scale_pos_weight':1.0,\n",
    "    'lambda_l2':0.1, 'lambda_l1':0.1, 'fair_c':1.0, 'bagging_freq':1,\n",
    "    'objective':'fair', 'seed':1, 'categorical_feature':0, 'xgboost_dart_mode':False,\n",
    "    'drop_rate':0.1, 'skip_drop':0.5, 'max_drop':50, 'top_rate':0.1, 'other_rate':0.1,\n",
    "    'max_bin':255, 'min_data_in_bin':50, 'bin_construct_sample_cnt':1000000,\n",
    "    'two_round':False, 'uniform_drop':False,'metric': 'mae'\n",
    "}\n",
    "\n",
    "lgbm_meta_model = LgbmWrapper(params=lgbm_meta_params, num_rounds = 2000, ealry_stopping=100,\n",
    "               verbose_eval=False, base_score=True, maximize=False, y_value_log=False)\n",
    "\n",
    "lgbm_cv_score, best_round = kfold_test(lgbm_meta_model, x_train, y_train, rmse, NFOLDS=9, kfold_random_sate=4950 )\n",
    "\n",
    "\n",
    "d_train_all = lgbm.Dataset(x_train, label=y_train)\n",
    "bst = lgbm.train(lgbm_meta_params, d_train_all ,best_round)\n",
    "#predictions = bst.predict(pd.DataFrame(x_test_second_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
