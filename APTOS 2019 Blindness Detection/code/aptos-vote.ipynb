{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voting from data\n",
    "\n",
    "1.https://www.kaggle.com/chanhu/eye-inference-num-class-1-ver3 \n",
    "\n",
    "2.https://www.kaggle.com/abhishek/pytorch-inference-kernel-lazy-tta\n",
    "\n",
    "3.https://www.kaggle.com/ratan123/aptos-2019-keras-baseline\n",
    "\n",
    "4.https://www.kaggle.com/drhabib/starter-kernel-for-0-79\n",
    "\n",
    "thanks for sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nvidiaapex', 'kaggle-public-copy', 'pretrained-models', 'aptos2019-blindness-detection', 'efficientnet', 'enet-test', 'efficientnet-pytorch', 'mmmodel', 'dr-model', 'densenet-keras']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.https://www.kaggle.com/chanhu/eye-inference-num-class-1-ver3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isfile\n",
    "import torch.nn.init as init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageFilter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import urllib\n",
    "import pickle\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [03:50,  1.65s/it]\n",
      "121it [03:48,  1.64s/it]\n",
      "121it [03:48,  1.73s/it]\n",
      "121it [03:47,  1.66s/it]\n",
      "121it [03:48,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "package_path = '../input/efficientnet/efficientnet-pytorch/EfficientNet-PyTorch/'\n",
    "sys.path.append(package_path)\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(1234)\n",
    "TTA         = 5\n",
    "num_classes = 1\n",
    "IMG_SIZE    = 256\n",
    "test = '../input/aptos2019-blindness-detection/test_images/'\n",
    "def expand_path(p):\n",
    "    p = str(p)\n",
    "    if isfile(test + p + \".png\"):\n",
    "        return test + (p + \".png\")\n",
    "    return p\n",
    "\n",
    "def p_show(imgs, label_name=None, per_row=3):\n",
    "    n = len(imgs)\n",
    "    rows = (n + per_row - 1)//per_row\n",
    "    cols = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows,cols, figsize=(15,15))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i,(p, ax) in enumerate(zip(imgs, axes.flatten())): \n",
    "        img = Image.open(expand_path(p))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(train_df[train_df.id_code == p].diagnosis.values)\n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.df.diagnosis.values[idx]\n",
    "        label = np.expand_dims(label, -1)\n",
    "        \n",
    "        p = self.df.id_code.values[idx]\n",
    "        p_path = expand_path(p)\n",
    "        image = cv2.imread(p_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = crop_image_from_gray(image)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , 30) ,-4 ,128)\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation((-120, 120)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "testset        = MyDataset(pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv'), \n",
    "                 transform=test_transform)\n",
    "test_loader    = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)\n",
    "model = EfficientNet.from_name('efficientnet-b0')\n",
    "in_features = model._fc.in_features\n",
    "model._fc = nn.Linear(in_features, num_classes)\n",
    "model.load_state_dict(torch.load('../input/enet-test/weight_best(3).pt'))\n",
    "model.cuda()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "sample = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "test_pred = np.zeros((len(sample), 1))\n",
    "model.eval()\n",
    "\n",
    "for _ in range(TTA):\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(test_loader)):\n",
    "            images, _ = data\n",
    "            images = images.cuda()\n",
    "            pred = model(images)\n",
    "            test_pred[i * 16:(i + 1) * 16] += pred.detach().cpu().squeeze().numpy().reshape(-1, 1)\n",
    "\n",
    "output = test_pred / TTA\n",
    "preds1 = output.copy()\n",
    "coef = [0.57, 1.37, 2.57, 3.57]\n",
    "for i, pred in enumerate(output):\n",
    "    if pred < coef[0]:\n",
    "        output[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        output[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        output[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        output[i] = 3\n",
    "    else:\n",
    "        output[i] = 4\n",
    "submission1 = pd.DataFrame({'id_code':pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv').id_code.values,\n",
    "                          'diagnosis':np.squeeze(output).astype(int)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.https://www.kaggle.com/abhishek/pytorch-inference-kernel-lazy-tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = \"../input/pretrained-models/pretrained-models/pretrained-models.pytorch-master/\"\n",
    "sys.path.insert(0, package_dir)\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:55<00:00,  1.11it/s]\n",
      "100%|██████████| 61/61 [00:55<00:00,  1.11it/s]\n",
      "100%|██████████| 61/61 [00:54<00:00,  2.33it/s]\n",
      "100%|██████████| 61/61 [00:54<00:00,  2.38it/s]\n",
      "100%|██████████| 61/61 [00:54<00:00,  1.12it/s]\n",
      "100%|██████████| 61/61 [00:53<00:00,  1.14it/s]\n",
      "100%|██████████| 61/61 [00:54<00:00,  1.13it/s]\n",
      "100%|██████████| 61/61 [00:53<00:00,  1.14it/s]\n",
      "100%|██████████| 61/61 [00:53<00:00,  2.53it/s]\n",
      "100%|██████████| 61/61 [00:53<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "class RetinopathyDatasetTest(Dataset):\n",
    "    def __init__(self, csv_file, transform):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join('../input/aptos2019-blindness-detection/test_images', self.data.loc[idx, 'id_code'] + '.png')\n",
    "        image = Image.open(img_name)\n",
    "        image = self.transform(image)\n",
    "        return {'image': image}\n",
    "model = pretrainedmodels.__dict__['resnet101'](pretrained=None)\n",
    "\n",
    "model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "model.last_linear = nn.Sequential(\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.25),\n",
    "                          nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.5),\n",
    "                          nn.Linear(in_features=2048, out_features=1, bias=True),\n",
    "                         )\n",
    "model.load_state_dict(torch.load(\"../input/mmmodel/model.bin\"))\n",
    "model = model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = RetinopathyDatasetTest(csv_file='../input/aptos2019-blindness-detection/sample_submission.csv',\n",
    "                                      transform=test_transform)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds1 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds1[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds2 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds2[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds3 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds3[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds4 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds4[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds5 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds5[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds6 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds6[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds7 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds7[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds8 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds8[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds9 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds9[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds10 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds10[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)\n",
    "test_preds = (test_preds1 + test_preds2 + test_preds3 + test_preds4 + test_preds5\n",
    "             + test_preds6 + test_preds7 + test_preds8 + test_preds9 + test_preds10) / 10.0\n",
    "coef = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "preds2 = test_preds.copy()\n",
    "for i, pred in enumerate(test_preds):\n",
    "    if pred < coef[0]:\n",
    "        test_preds[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        test_preds[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        test_preds[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        test_preds[i] = 3\n",
    "    else:\n",
    "        test_preds[i] = 4\n",
    "submission2 = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "submission2.diagnosis = test_preds.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.https://www.kaggle.com/ratan123/aptos-2019-keras-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.densenet import DenseNet121,DenseNet169\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score, cohen_kappa_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imgaug as ia\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 300\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24/25 [===========================>..] - ETA: 6s - loss: 1.5624 \n",
      "25/25 [==============================] - 164s 7s/step - loss: 1.5377\n",
      "18/18 [==============================] - 86s 5s/step\n",
      "\n",
      " epoch: 1 - QWK_score: 0.010223 \n",
      "\n",
      "saving checkpoint:  0.010222870291698882\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 91s 4s/step - loss: 0.8822\n",
      "18/18 [==============================] - 83s 5s/step\n",
      "\n",
      " epoch: 2 - QWK_score: 0.011114 \n",
      "\n",
      "saving checkpoint:  0.011113531870545756\n",
      "Epoch 1/30\n",
      "98/98 [==============================] - 322s 3s/step - loss: 0.8430 - val_loss: 0.6811\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68107, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 1 - QWK_score: 0.702260 \n",
      "\n",
      "saving checkpoint:  0.7022600258385987\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 269s 3s/step - loss: 0.7159 - val_loss: 0.5280\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68107 to 0.52796, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 2 - QWK_score: 0.867927 \n",
      "\n",
      "saving checkpoint:  0.8679271289414572\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 270s 3s/step - loss: 0.6683 - val_loss: 0.4760\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52796 to 0.47598, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 60s 3s/step\n",
      "\n",
      " epoch: 3 - QWK_score: 0.885271 \n",
      "\n",
      "saving checkpoint:  0.885270671011535\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 273s 3s/step - loss: 0.6233 - val_loss: 0.4580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47598 to 0.45799, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 59s 3s/step\n",
      "\n",
      " epoch: 4 - QWK_score: 0.876759 \n",
      "\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 272s 3s/step - loss: 0.6229 - val_loss: 0.4876\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.45799\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 5 - QWK_score: 0.880169 \n",
      "\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 271s 3s/step - loss: 0.5850 - val_loss: 0.4652\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45799\n",
      "18/18 [==============================] - 60s 3s/step\n",
      "\n",
      " epoch: 6 - QWK_score: 0.895431 \n",
      "\n",
      "saving checkpoint:  0.8954312313479865\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 272s 3s/step - loss: 0.5485 - val_loss: 0.4648\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45799\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 7 - QWK_score: 0.872253 \n",
      "\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 273s 3s/step - loss: 0.5642 - val_loss: 0.5320\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45799\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 8 - QWK_score: 0.859007 \n",
      "\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 274s 3s/step - loss: 0.5084 - val_loss: 0.5117\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45799\n",
      "18/18 [==============================] - 63s 4s/step\n",
      "\n",
      " epoch: 9 - QWK_score: 0.868679 \n",
      "\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 277s 3s/step - loss: 0.5122 - val_loss: 0.4442\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.45799 to 0.44424, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 63s 4s/step\n",
      "\n",
      " epoch: 10 - QWK_score: 0.898778 \n",
      "\n",
      "saving checkpoint:  0.89877784411788\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 276s 3s/step - loss: 0.4726 - val_loss: 0.4491\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44424\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 11 - QWK_score: 0.891755 \n",
      "\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 275s 3s/step - loss: 0.4705 - val_loss: 0.4325\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.44424 to 0.43253, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 12 - QWK_score: 0.914701 \n",
      "\n",
      "saving checkpoint:  0.9147005563577029\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - 277s 3s/step - loss: 0.4676 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.43253 to 0.43061, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 13 - QWK_score: 0.903253 \n",
      "\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - 275s 3s/step - loss: 0.4593 - val_loss: 0.4274\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.43061 to 0.42736, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 14 - QWK_score: 0.905885 \n",
      "\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - 274s 3s/step - loss: 0.4424 - val_loss: 0.4451\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42736\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 15 - QWK_score: 0.902880 \n",
      "\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - 277s 3s/step - loss: 0.4471 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.42736\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 16 - QWK_score: 0.909142 \n",
      "\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - 274s 3s/step - loss: 0.4422 - val_loss: 0.5418\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42736\n",
      "18/18 [==============================] - 60s 3s/step\n",
      "\n",
      " epoch: 17 - QWK_score: 0.898139 \n",
      "\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - 276s 3s/step - loss: 0.4191 - val_loss: 0.4565\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42736\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 18 - QWK_score: 0.888535 \n",
      "\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - 277s 3s/step - loss: 0.4003 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.42736 to 0.42534, saving model to ../working/densenet_.h5\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 19 - QWK_score: 0.915223 \n",
      "\n",
      "saving checkpoint:  0.9152226022728941\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - 270s 3s/step - loss: 0.3985 - val_loss: 0.4362\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 20 - QWK_score: 0.912649 \n",
      "\n",
      "Epoch 21/30\n",
      "98/98 [==============================] - 270s 3s/step - loss: 0.3918 - val_loss: 0.4584\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 21 - QWK_score: 0.904390 \n",
      "\n",
      "Epoch 22/30\n",
      "98/98 [==============================] - 270s 3s/step - loss: 0.3849 - val_loss: 0.4329\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 62s 3s/step\n",
      "\n",
      " epoch: 22 - QWK_score: 0.912901 \n",
      "\n",
      "Epoch 23/30\n",
      "98/98 [==============================] - 272s 3s/step - loss: 0.3891 - val_loss: 0.4350\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.42534\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "18/18 [==============================] - 59s 3s/step\n",
      "\n",
      " epoch: 23 - QWK_score: 0.912360 \n",
      "\n",
      "Epoch 24/30\n",
      "98/98 [==============================] - 271s 3s/step - loss: 0.3648 - val_loss: 0.4393\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 24 - QWK_score: 0.914461 \n",
      "\n",
      "Epoch 25/30\n",
      "98/98 [==============================] - 267s 3s/step - loss: 0.3670 - val_loss: 0.4353\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 25 - QWK_score: 0.917330 \n",
      "\n",
      "saving checkpoint:  0.9173299277064472\n",
      "Epoch 26/30\n",
      "98/98 [==============================] - 270s 3s/step - loss: 0.3504 - val_loss: 0.4749\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 61s 3s/step\n",
      "\n",
      " epoch: 26 - QWK_score: 0.903564 \n",
      "\n",
      "Epoch 27/30\n",
      "98/98 [==============================] - 271s 3s/step - loss: 0.3410 - val_loss: 0.4646\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.42534\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "18/18 [==============================] - 60s 3s/step\n",
      "\n",
      " epoch: 27 - QWK_score: 0.909142 \n",
      "\n",
      "Epoch 28/30\n",
      "98/98 [==============================] - 269s 3s/step - loss: 0.3506 - val_loss: 0.4500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.42534\n",
      "18/18 [==============================] - 60s 3s/step\n",
      "\n",
      " epoch: 28 - QWK_score: 0.910736 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1928it [03:19,  9.65it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=8)\n",
    "y = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=8)\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "        [\n",
    "            # apply the following augmenters to most images\n",
    "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.2), # vertically flip 20% of all images\n",
    "            sometimes(iaa.Affine(\n",
    "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
    "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n",
    "                rotate=(-10, 10), # rotate by -45 to +45 degrees\n",
    "                shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "                cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
    "                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "            )),\n",
    "            # execute 0 to 5 of the following (less important) augmenters per image\n",
    "            # don't execute all of them, as that would often be way too strong\n",
    "            iaa.SomeOf((0, 5),\n",
    "                [\n",
    "                    sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
    "                    iaa.OneOf([\n",
    "                        iaa.GaussianBlur((0, 1.0)), # blur images with a sigma between 0 and 3.0\n",
    "                        iaa.AverageBlur(k=(3, 5)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                        iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                    ]),\n",
    "                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), # sharpen images\n",
    "                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
    "                    # search either for all edges or for directed edges,\n",
    "                    # blend the result with the original image using a blobby mask\n",
    "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                    ])),\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Dropout((0.01, 0.05), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                        iaa.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),\n",
    "                    ]),\n",
    "                    iaa.Invert(0.01, per_channel=True), # invert color channels\n",
    "                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation\n",
    "                    # either change the brightness of the whole image (sometimes\n",
    "                    # per channel) or change the brightness of subareas\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
    "                        iaa.FrequencyNoiseAlpha(\n",
    "                            exponent=(-1, 0),\n",
    "                            first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                            second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                        )\n",
    "                    ]),\n",
    "                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n",
    "                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "        ],\n",
    "        random_order=True)\n",
    "class My_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels,\n",
    "                 batch_size, is_train=True,\n",
    "                 mix=False, augment=False):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.is_train = is_train\n",
    "        self.is_augment = augment\n",
    "        if(self.is_train):\n",
    "            self.on_epoch_end()\n",
    "        self.is_mix = mix\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if(self.is_train):\n",
    "            return self.train_generate(batch_x, batch_y)\n",
    "        return self.valid_generate(batch_x, batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if(self.is_train):\n",
    "            self.image_filenames, self.labels = shuffle(self.image_filenames, self.labels)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def mix_up(self, x, y):\n",
    "        lam = np.random.beta(0.2, 0.4)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "    def train_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (sample, label) in zip(batch_x, batch_y):\n",
    "            img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+sample+'.png')\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            if(self.is_augment):\n",
    "                img = seq.augment_image(img)\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        if(self.is_mix):\n",
    "            batch_images, batch_y = self.mix_up(batch_images, batch_y)\n",
    "        return batch_images, batch_y\n",
    "\n",
    "    def valid_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (sample, label) in zip(batch_x, batch_y):\n",
    "            img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+sample+'.png')\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        return batch_images, batch_y\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = DenseNet121(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights(\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\")\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation='softmax', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output) \n",
    "    return model\n",
    "# create callbacks list\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "\n",
    "epochs = 30; batch_size = 32\n",
    "checkpoint = ModelCheckpoint('../working/densenet_.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "\n",
    "csv_logger = CSVLogger(filename='../working/training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "train_generator = My_Generator(train_x, train_y, 128, is_train=True)\n",
    "train_mixup = My_Generator(train_x, train_y, batch_size, is_train=True, mix=False, augment=True)\n",
    "valid_generator = My_Generator(valid_x, valid_y, batch_size, is_train=False)\n",
    "\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=NUM_CLASSES)\n",
    "# reference link: https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow\n",
    "def kappa_loss(y_true, y_pred, y_pow=2, eps=1e-12, N=5, bsize=32, name='kappa'):\n",
    "    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n",
    "        Args:\n",
    "            y_pred: 2D tensor or array, [batch_size, num_classes]\n",
    "            y_true: 2D tensor or array,[batch_size, num_classes]\n",
    "            y_pow: int,  e.g. y_pow=2\n",
    "            N: typically num_classes of the model\n",
    "            bsize: batch_size of the training or validation ops\n",
    "            eps: a float, prevents divide by zero\n",
    "            name: Optional scope/name for op_scope.\n",
    "        Returns:\n",
    "            A tensor with the kappa loss.\"\"\"\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom*0.5 / (denom + eps) + categorical_crossentropy(y_true, y_pred)*0.5\n",
    "from keras.callbacks import Callback\n",
    "class QWKEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), batch_size=64, interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_generator, self.y_val = validation_data\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_generator(generator=self.valid_generator,\n",
    "                                                  steps=np.ceil(float(len(self.y_val)) / float(self.batch_size)),\n",
    "                                                  workers=1, use_multiprocessing=False,\n",
    "                                                  verbose=1)\n",
    "            def flatten(y):\n",
    "                return np.argmax(y, axis=1).reshape(-1)\n",
    "            \n",
    "            score = cohen_kappa_score(flatten(self.y_val),\n",
    "                                      flatten(y_pred),\n",
    "                                      labels=[0,1,2,3,4],\n",
    "                                      weights='quadratic')\n",
    "            print(\"\\n epoch: %d - QWK_score: %.6f \\n\" % (epoch+1, score))\n",
    "            self.history.append(score)\n",
    "            if score >= max(self.history):\n",
    "                print('saving checkpoint: ', score)\n",
    "                self.model.save('../working/densenet_bestqwk.h5')\n",
    "\n",
    "qwk = QWKEvaluation(validation_data=(valid_generator, valid_y),\n",
    "                    batch_size=batch_size, interval=1)\n",
    "# warm up model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-3,0):\n",
    "    model.layers[i].trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-3))\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_y)) / float(128)),\n",
    "    epochs=2,\n",
    "    workers=WORKERS, use_multiprocessing=True,\n",
    "    verbose=1,\n",
    "    callbacks=[qwk])\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "callbacks_list = [checkpoint, csv_logger, reduceLROnPlat, early, qwk]\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            # loss=kappa_loss,\n",
    "            optimizer=Adam(lr=1e-4))\n",
    "model.fit_generator(\n",
    "    train_mixup,\n",
    "    steps_per_epoch=np.ceil(float(len(train_x)) / float(batch_size)),\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_x)) / float(batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    workers=1, use_multiprocessing=False,\n",
    "    callbacks=callbacks_list)\n",
    "submission3 = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "model.load_weights('../working/densenet_bestqwk.h5')\n",
    "predicted = []\n",
    "# reference:https://www.kaggle.com/CVxTz/cnn-starter-nasnet-mobile-0-9709-lb \n",
    "for i, name in tqdm(enumerate(submission3['id_code'])):\n",
    "    path = os.path.join('../input/aptos2019-blindness-detection/test_images/', name+'.png')\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (SIZE, SIZE))\n",
    "    X = np.array((image[np.newaxis])/255)\n",
    "    score_predict=((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
    "    label_predict = np.argmax(score_predict)\n",
    "    predicted.append(label_predict)\n",
    "submission3['diagnosis'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.https://www.kaggle.com/drhabib/starter-kernel-for-0-79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from torchvision import models as md\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "from torch.utils import model_zoo\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the entire model (stem, all blocks, and head)\n",
    "GlobalParams = collections.namedtuple('GlobalParams', [\n",
    "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
    "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
    "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
    "\n",
    "\n",
    "# Parameters for an individual model block\n",
    "BlockArgs = collections.namedtuple('BlockArgs', [\n",
    "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
    "    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
    "\n",
    "\n",
    "# Change namedtuple defaults\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "\n",
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def round_filters(filters, global_params):\n",
    "    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n",
    "    multiplier = global_params.width_coefficient\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_same_padding_conv2d(image_size=None):\n",
    "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
    "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
    "    if image_size is None:\n",
    "        return Conv2dDynamicSamePadding\n",
    "    else:\n",
    "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "\n",
    "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class Conv2dStaticSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "        # Calculate padding based on image size and save it\n",
    "        assert image_size is not None\n",
    "        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "        else:\n",
    "            self.static_padding = Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.static_padding(x)\n",
    "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "########################################################################\n",
    "############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def efficientnet_params(model_name):\n",
    "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
    "    params_dict = {\n",
    "        # Coefficients:   width,depth,res,dropout\n",
    "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "    }\n",
    "    return params_dict[model_name]\n",
    "\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_block_string(block_string):\n",
    "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "\n",
    "        ops = block_string.split('_')\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r'(\\d.*)', op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "\n",
    "        # Check stride\n",
    "        assert (('s' in options and len(options['s']) == 1) or\n",
    "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
    "\n",
    "        return BlockArgs(\n",
    "            kernel_size=int(options['k']),\n",
    "            num_repeat=int(options['r']),\n",
    "            input_filters=int(options['i']),\n",
    "            output_filters=int(options['o']),\n",
    "            expand_ratio=int(options['e']),\n",
    "            id_skip=('noskip' not in block_string),\n",
    "            se_ratio=float(options['se']) if 'se' in options else None,\n",
    "            stride=[int(options['s'][0])])\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_block_string(block):\n",
    "        \"\"\"Encodes a block to a string.\"\"\"\n",
    "        args = [\n",
    "            'r%d' % block.num_repeat,\n",
    "            'k%d' % block.kernel_size,\n",
    "            's%d%d' % (block.strides[0], block.strides[1]),\n",
    "            'e%s' % block.expand_ratio,\n",
    "            'i%d' % block.input_filters,\n",
    "            'o%d' % block.output_filters\n",
    "        ]\n",
    "        if 0 < block.se_ratio <= 1:\n",
    "            args.append('se%s' % block.se_ratio)\n",
    "        if block.id_skip is False:\n",
    "            args.append('noskip')\n",
    "        return '_'.join(args)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(string_list):\n",
    "        \"\"\"\n",
    "        Decodes a list of string notations to specify blocks inside the network.\n",
    "\n",
    "        :param string_list: a list of strings, each string is a notation of block\n",
    "        :return: a list of BlockArgs namedtuples of block args\n",
    "        \"\"\"\n",
    "        assert isinstance(string_list, list)\n",
    "        blocks_args = []\n",
    "        for block_string in string_list:\n",
    "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
    "        return blocks_args\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(blocks_args):\n",
    "        \"\"\"\n",
    "        Encodes a list of BlockArgs to a list of strings.\n",
    "\n",
    "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
    "        :return: a list of strings, each string is a notation of block\n",
    "        \"\"\"\n",
    "        block_strings = []\n",
    "        for block in blocks_args:\n",
    "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
    "        return block_strings\n",
    "\n",
    "\n",
    "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
    "    \"\"\" Creates a efficientnet model. \"\"\"\n",
    "\n",
    "    blocks_args = [\n",
    "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
    "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "    ]\n",
    "    blocks_args = BlockDecoder.decode(blocks_args)\n",
    "\n",
    "    global_params = GlobalParams(\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        dropout_rate=dropout_rate,\n",
    "        drop_connect_rate=drop_connect_rate,\n",
    "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "        image_size=image_size,\n",
    "    )\n",
    "\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "def get_model_params(model_name, override_params):\n",
    "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
    "    if model_name.startswith('efficientnet'):\n",
    "        w, d, s, p = efficientnet_params(model_name)\n",
    "        # note: all models have drop connect rate = 0.2\n",
    "        blocks_args, global_params = efficientnet(\n",
    "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
    "    else:\n",
    "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
    "    if override_params:\n",
    "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
    "        global_params = global_params._replace(**override_params)\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "url_map = {\n",
    "    'efficientnet-b0': 'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth',\n",
    "    'efficientnet-b1': 'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth',\n",
    "    'efficientnet-b2': 'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth',\n",
    "    'efficientnet-b3': 'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth',\n",
    "    'efficientnet-b4': 'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth',\n",
    "    'efficientnet-b5': 'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth',\n",
    "}\n",
    "\n",
    "def load_pretrained_weights(model, model_name, load_fc=True):\n",
    "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "    state_dict = model_zoo.load_url(url_map[model_name])\n",
    "    if load_fc:\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        state_dict.pop('_fc.weight')\n",
    "        state_dict.pop('_fc.bias')\n",
    "        res = model.load_state_dict(state_dict, strict=False)\n",
    "        assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))\n",
    "    \n",
    "    \n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, see above\n",
    "        global_params (namedtuple): GlobalParam, see above\n",
    "\n",
    "    Attributes:\n",
    "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Expansion phase\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
    "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = self._block_args.output_filters\n",
    "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"\n",
    "        :param inputs: input tensor\n",
    "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "        :return: output of block\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
    "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
    "\n",
    "    Args:\n",
    "        blocks_args (list): A list of BlockArgs to construct blocks\n",
    "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
    "\n",
    "    Example:\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blocks_args=None, global_params=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "        self._global_params = global_params\n",
    "        self._blocks_args = blocks_args\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
    "        bn_eps = self._global_params.batch_norm_epsilon\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3  # rgb\n",
    "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self._blocks_args:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "            if block_args.num_repeat > 1:\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self._global_params)\n",
    "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = self._global_params.dropout_rate\n",
    "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "\n",
    "        # Head\n",
    "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs)\n",
    "\n",
    "        # Pooling and final linear layer\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        if self._dropout:\n",
    "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
    "        x = self._fc(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, model_name, override_params=None):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
    "        return EfficientNet(blocks_args, global_params)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, num_classes=1000):\n",
    "        model = EfficientNet.from_name(model_name, override_params={'num_classes': num_classes})\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def get_image_size(cls, model_name):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n",
    "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
    "        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n",
    "        num_models = 4 if also_need_pretrained_weights else 8\n",
    "        valid_models = ['efficientnet_b'+str(i) for i in range(num_models)]\n",
    "        if model_name.replace('-','_') not in valid_models:\n",
    "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))\n",
    "#making model\n",
    "md_ef = EfficientNet.from_pretrained('efficientnet-b5', num_classes=1)\n",
    "#copying weighst to the local directory \n",
    "!mkdir models\n",
    "!cp '../input/kaggle-public-copy/abcdef.pth' 'models'\n",
    "def get_df():\n",
    "    base_image_dir = os.path.join('..', 'input/aptos2019-blindness-detection/')\n",
    "    train_dir = os.path.join(base_image_dir,'train_images/')\n",
    "    df = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\n",
    "    df['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\n",
    "    df = df.drop(columns=['id_code'])\n",
    "    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
    "    test_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "    return df, test_df\n",
    "\n",
    "df, test_df = get_df()\n",
    "#you can play around with tfms and image sizes\n",
    "bs = 64\n",
    "sz = 224\n",
    "tfms = get_transforms(do_flip=True,flip_vert=True)\n",
    "data = (ImageList.from_df(df=df,path='./',cols='path') \n",
    "        .split_by_rand_pct(0.2) \n",
    "        .label_from_df(cols='diagnosis',label_cls=FloatList) \n",
    "        .transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') \n",
    "        .databunch(bs=bs,num_workers=4) \n",
    "        .normalize(imagenet_stats)  \n",
    "       )\n",
    "def qk(y_pred, y):\n",
    "    return torch.tensor(cohen_kappa_score(torch.round(y_pred), y, weights='quadratic'), device='cuda:0')\n",
    "learn = Learner(data, \n",
    "                md_ef, \n",
    "                metrics = [qk], \n",
    "                model_dir=\"models\").to_fp16()\n",
    "\n",
    "learn.data.add_test(ImageList.from_df(test_df,\n",
    "                                      '../input/aptos2019-blindness-detection',\n",
    "                                      folder='test_images',\n",
    "                                      suffix='.png'))\n",
    "learn.load('abcdef');\n",
    "#https://www.kaggle.com/abhishek/optimizer-for-quadratic-weighted-kappa\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "        print(-loss_partial(self.coef_['x']))\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "coefficients=[0.5, 1.5, 2.5, 3.5]\n",
    "opt = OptimizedRounder()\n",
    "preds4,y = learn.get_preds(DatasetType.Test)\n",
    "tst_pred = opt.predict(preds4, coefficients)\n",
    "test_df.diagnosis = tst_pred.astype(int)\n",
    "submission4 = test_df.copy()\n",
    "submission4.to_csv('submission4.csv',index=False)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "score = [0.777, 0.758, 0.749, 0.783]\n",
    "weight = [0.3, 0.19, 0.11, 0.40]\n",
    "subData = [submission1, submission2, submission3, submission4]\n",
    "predsData = [preds1, preds2, preds4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.11 0.89 0.   0.  ]\n",
      " [0.   0.   0.41 0.59 0.  ]\n",
      " [0.   0.   0.6  0.4  0.  ]\n",
      " [0.   0.   1.   0.   0.  ]\n",
      " ...\n",
      " [0.   0.   1.   0.   0.  ]\n",
      " [0.   0.   0.51 0.49 0.  ]\n",
      " [0.   0.   0.   0.89 0.11]\n",
      " [0.11 0.19 0.7  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "numClass = 5\n",
    "subTemp = np.zeros((subData[0].shape[0],numClass))\n",
    "for i in range(len(subData)):\n",
    "    subTemp[subData[i].index, subData[i].diagnosis.tolist()] += weight[i]\n",
    "print(subTemp)\n",
    "sub = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "sub['diagnosis'] = subTemp.argmax(1).astype(int)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = [0.777, 0.758, 0.783]\n",
    "# weight = [0.35, 0.2, 0.45]\n",
    "# predsData = weight[0]*preds1 + weight[1]*preds2 + weight[2]*preds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef = [0.5, 1.5, 2.5, 3.5]\n",
    "# for i, pred in enumerate(predsData):\n",
    "#     if pred < coef[0]:\n",
    "#         predsData[i] = 0\n",
    "#     elif pred >= coef[0] and pred < coef[1]:\n",
    "#         predsData[i] = 1\n",
    "#     elif pred >= coef[1] and pred < coef[2]:\n",
    "#         predsData[i] = 2\n",
    "#     elif pred >= coef[2] and pred < coef[3]:\n",
    "#         predsData[i] = 3\n",
    "#     else:\n",
    "#         predsData[i] = 4\n",
    "# submission = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "# submission.diagnosis = predsData.astype(int)\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
